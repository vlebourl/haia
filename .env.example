# HAIA Configuration Template
# Copy this file to .env and fill in your actual values

# Anthropic API Configuration
ANTHROPIC_API_KEY=sk-ant-api03-your-key-here

# Model Configuration
# Options: anthropic:claude-haiku-4-5-20251001, anthropic:claude-sonnet-4-5-20250929, ollama:qwen2.5-coder
HAIA_MODEL=anthropic:claude-haiku-4-5-20251001

# System Prompt Configuration (REQUIRED - copy from your .env file)
# This defines Haia's personality, behavior, and capabilities.
# Copy the full HAIA_SYSTEM_PROMPT from your actual .env file here.
# The comprehensive prompt should be a multi-line string covering:
# - Who Haia is (personality, communication style)
# - Technical expertise areas
# - Safety protocols and operational guidelines
# - Example interactions
#
# If not set, falls back to a minimal default prompt (not recommended for production).
# See your .env file for the full prompt - do not commit your personalized version here.
HAIA_SYSTEM_PROMPT="[Copy your comprehensive system prompt from .env here]"

# Homelab Profile Configuration (OPTIONAL)
# Path to your personal homelab profile YAML file
# Default: haia_profile.yaml (automatically gitignored)
HAIA_PROFILE_PATH=haia_profile.yaml

# Conversation Settings
# Number of messages to keep in conversation context
# Claude Haiku 4.5 (200k tokens): Recommended 100 messages (~50k tokens, 25% utilization)
# Claude Sonnet 4.5 (200k tokens): Recommended 100 messages
# Ollama models: Adjust based on model's context window (typically 4k-32k tokens)
CONTEXT_WINDOW_SIZE=100

# Database Configuration (Conversation history - Session 2)
DATABASE_URL=sqlite+aiosqlite:///./haia.db

# Neo4j Memory Database Configuration (Session 3)
# Connection URI - use 'neo4j' hostname for Docker, 'localhost' for local development
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_secure_neo4j_password_here

# Memory Extraction Configuration (Session 7)
# Model for memory extraction - defaults to HAIA_MODEL if not specified
# Using a lighter model (like Haiku) for extraction is cost-effective
EXTRACTION_MODEL=
# Minimum confidence threshold for extracted memories (0.0-1.0)
# Default: 0.4 (selective/aggressive strategy)
EXTRACTION_MIN_CONFIDENCE=0.4

# Embedding Configuration (Session 8 - Memory Retrieval)
# Embedding model for semantic search - using Ollama for privacy and consistency
# Options: ollama:nomic-embed-text (768-dim, 62% MTEB, recommended), ollama:mxbai-embed-large (1024-dim, 65% MTEB)
EMBEDDING_MODEL=ollama:nomic-embed-text
EMBEDDING_DIM=768
# Ollama API base URL - use 'http://ollama:11434' for Docker, 'http://localhost:11434' for local
OLLAMA_BASE_URL=http://localhost:11434

# Retrieval Configuration (Session 8 - Memory Retrieval)
# Number of memories to retrieve in semantic search
RETRIEVAL_TOP_K=10
# Minimum cosine similarity threshold (0.0-1.0) - higher = stricter matching
RETRIEVAL_MIN_SIMILARITY=0.65
# Minimum confidence threshold (0.0-1.0) for retrieved memories
RETRIEVAL_MIN_CONFIDENCE=0.4

# Memory Type Weights (Session 8 - User Story 3: Relevance Filtering)
# Multipliers for relevance scoring by memory type (0.0-10.0)
# Higher values prioritize that memory type in retrieval results
MEMORY_TYPE_WEIGHT_PREFERENCE=1.2
MEMORY_TYPE_WEIGHT_TECHNICAL_CONTEXT=1.1
MEMORY_TYPE_WEIGHT_DECISION=1.0
MEMORY_TYPE_WEIGHT_PERSONAL_FACT=0.9
MEMORY_TYPE_WEIGHT_CORRECTION=1.3

# Server Configuration
HOST=0.0.0.0
PORT=8000
